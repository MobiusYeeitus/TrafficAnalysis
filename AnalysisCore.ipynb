{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### System Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "import decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "except ModuleNotFoundError:\n",
    "    os.system('pip install numpy')\n",
    "    import numpy as np\n",
    "try:\n",
    "    import scapy\n",
    "except ModuleNotFoundError:\n",
    "    os.system('pip install scapy')\n",
    "    import scapy\n",
    "try:\n",
    "    import tqdm\n",
    "except ModuleNotFoundError:\n",
    "    os.system('pip install tqdm')\n",
    "    import tqdm\n",
    "try:\n",
    "    import requests\n",
    "except ModuleNotFoundError:\n",
    "    os.system('pip install requests')\n",
    "    import requests\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError:\n",
    "    os.system('pip install pandas')\n",
    "    import pandas as pd\n",
    "try:\n",
    "    import sklearn\n",
    "except ModuleNotFoundError:\n",
    "    os.system('pip install scikit-learn')\n",
    "    import sklearn\n",
    "try:\n",
    "    import matplotlib\n",
    "except ModuleNotFoundError:\n",
    "    os.system('pip install matplotlib')\n",
    "    import matplotlib\n",
    "    \n",
    "logging.getLogger(\"requests\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans,DBSCAN,AgglomerativeClustering,MeanShift,SpectralClustering,AffinityPropagation,Birch,OPTICS\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "from scapy.all import rdpcap\n",
    "from tqdm import trange,tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzer\n",
    "\n",
    "#### Minor Function Breakdowns\n",
    "---\n",
    "##### Generate_Logger\n",
    "Generates a logger with default parameters for logging critical components and debugging, default mode is INFO, for debugging change it to DEBUG\n",
    "##### Generate Self IPs\n",
    "Generates a list of IPs assigned to current device according to a protocol list. Uses 'ipconfig' from command prompt to get info.\n",
    "##### List Initializer [V2]\n",
    "Helper function to shorten and simplify initializing and re-initializing lists with values. Avoids creating lists within lists if used correctly.\n",
    "##### Read JSON [V2] [DEPRECATED]\n",
    "Reads a JSON file from the provided directory with the filename.(extension). Can work without the filename extension, will throw error if file doesn't exist.\n",
    "##### Read List [V2] \n",
    "Reads a PKL file from the provided directory with the filename.(extension). Can work without the filename extension, will throw error if file doesn't exist.\n",
    "##### Write JSON [V2] [DEPRECATED]\n",
    "Creates a json file with the dictionary at the directory. Can work without filename extenstion.\n",
    "##### Write List [V2]\n",
    "Creates a pkl file with the dictionary at the directory. Can work without filename extenstion.\n",
    "##### Remove Reserved IPs\n",
    "Removes IPs that belong in the reserved ranges from the provided list. '192.168.' range is excluded.\n",
    "##### Calc Trans time\n",
    "Calculates the capture time of the pcap data\n",
    "##### PCAP_Input [1.5]\n",
    "Takes a PCAP file as input and stores it internally for later use\n",
    "##### Generate Unique IPs [V2]\n",
    "Generates a list of unique IPs found in the PCAP data (excluding self IPs), if already generated and saved, loads it. Can be forced.\n",
    "##### Analyze Flow [V1.7]\n",
    "##### Query IP\n",
    "Queries the ip-api website for a single ip, waits for the response, and returns it back. Primarily a helper function.\n",
    "##### Query IPs [V2]\n",
    "Requests IP information from ip-api through requests, and stores the received data in a list, saves the list so we dont have to query every single time we run the program, saving time. Can be forced.\n",
    "##### Generate Dataset [V1.2]\n",
    "Should generate a dataset for use in clustering\n",
    "##### Trace IPs\n",
    "Traces the IPs and returns the output\n",
    "##### Generate Summary [V1.1] [DEPRECATED]\n",
    "Summarizes the data in a neater form from queried data for the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Known Bugs\n",
    "\n",
    "Will throw a 'Flow' KeyError if the queried_data and unique_ips dont match\n",
    "\n",
    "\n",
    "Notes\n",
    "\n",
    "IAT List must be in normal python float else numpy will throw error because it cannon handle Decimal data\n",
    "\n",
    "if IP is assigned through DHCP, self_ip assignment and pcap_data self_ip may differ and show every field with default values\n",
    "\n",
    "To-Do\n",
    "\n",
    "-Empty-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Analyzer:\n",
    "    if True:\n",
    "        #Initialization of the logger, have to do it here, else it breaks stuff\n",
    "        logger=logging.getLogger\n",
    "        stored_logger_level=[logging.INFO]\n",
    "        stored_self_ip_protocol_list=[\"IPv4\",\"IPv6\"]\n",
    "\n",
    "        stored_pcap_data=[]\n",
    "        stored_path=[]\n",
    "        stored_directory=[]\n",
    "        \n",
    "        stored_transmission_time=[]\n",
    "        \n",
    "        stored_self_ips=[]\n",
    "        stored_unique_ips=[]\n",
    "        stored_blocked_ips=[]\n",
    "\n",
    "        stored_queried_data=[]\n",
    "        stored_flow=[]\n",
    "        stored_protocol_dict={'1':'ICMP','2':'IGMP','3':'GGP','4':'IPv4','5':'Stream','6':'TCP','17':'UDP'}\n",
    "        stored_port_dict={'53':'DNS','80':'HTTP','137':'NetBIOS','443':'HTTPS','1900':'SSDP','4070':'Amzn EDot2Sptfy','5222':'XMPP','22101':'GImpact','23301':'HSR'}\n",
    "        \n",
    "        stored_df=[pd.DataFrame()]\n",
    "        \n",
    "    def __init__(self,path,protocol_list=stored_self_ip_protocol_list,features=['Burst Down','Burst Up'],\n",
    "                 input=True,remove_reserved=True,\n",
    "                 query=True,forced_query=False,infinite_query=False,query_amount=45,\n",
    "                 replace=True,ping=True,cluster=True,clustering_algorithm='kmeans',clusters=6,samples=2,\n",
    "                 forced=False,verbose=True):\n",
    "        if True:\n",
    "            self.logger=self.Generate_Logger(filename='AnalysisCore.log')\n",
    "            logging.getLogger(\"requests\").setLevel(logging.INFO)\n",
    "            np.seterr(invalid='ignore')\n",
    "            self.Generate_Self_IPs(protocol_list=protocol_list)\n",
    "            directory=path[:path.find('.')]\n",
    "            self.List_Initializer(self.stored_path,path)\n",
    "            self.List_Initializer(self.stored_directory,directory)\n",
    "            if input:\n",
    "                self.PCAP_Input(path)\n",
    "            self.Calc_Trans_Time(forced=forced,verbose=verbose)\n",
    "            self.Generate_Unique_IPs(remove_reserved=remove_reserved,forced=forced,verbose=verbose)\n",
    "            self.Analyze_Flow(forced=forced,verbose=verbose)\n",
    "            if query==True:\n",
    "                self.Query_IPs(amount=query_amount,forced=forced_query,verbose=verbose)\n",
    "                if infinite_query:\n",
    "                    while len(self.stored_unique_ips)>len(self.stored_queried_data):\n",
    "                        self.Query_IPs(amount=query_amount,verbose=verbose)\n",
    "            self.Generate_Dataset(forced=forced,verbose=verbose,replace=replace,ping=ping)\n",
    "            if cluster:\n",
    "                self.Generate_Cluster(features=features,clustering_algorithm=clustering_algorithm,clusters=clusters,samples=samples)\n",
    "    def Generate_Logger(self,filename='default_logger_name.log'): \n",
    "        logging.basicConfig(filename=filename,format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "                            datefmt='%Y-%m-%d %H:%M:%S',filemode='w',level=self.stored_logger_level[0])\n",
    "        logger=logging.getLogger()\n",
    "        logger.debug('Logger initialized.')\n",
    "        return logger\n",
    "    def Generate_Self_IPs(self,protocol_list=stored_self_ip_protocol_list):\n",
    "        self.logger.info(\"Generating self IPs with protocols, \"+str(protocol_list))\n",
    "        self_ips=[]\n",
    "        process=subprocess.Popen(['ipconfig'],stdout=subprocess.PIPE,text=True,shell=True)\n",
    "        for line in process.stdout:\n",
    "            for protocol in protocol_list:\n",
    "                if re.search(protocol,line.strip()):\n",
    "                    self_ips.append(line.strip()[line.strip().find(':')+2:])\n",
    "        self.List_Initializer(self.stored_self_ips,self_ips)\n",
    "        self.logger.debug('Stored self IPs in stored_self_ips '+str(self_ips))\n",
    "        return self_ips\n",
    "    def List_Initializer(self,input_list,input_param):\n",
    "        if isinstance(input_param,list) or isinstance(input_param,scapy.plist.PacketList):\n",
    "            input_list.clear()\n",
    "            for item in input_param:\n",
    "                input_list.append(item)\n",
    "        else:\n",
    "            input_list.clear()\n",
    "            input_list.append(input_param)\n",
    "    def Dec_Trunc(self,input_decimal,decimal=100000):\n",
    "        return math.trunc(input_decimal*decimal)/decimal\n",
    "\n",
    "    def Read_JSON(self,directory,filename): #deprecated\n",
    "        name, ext=os.path.splitext(filename)\n",
    "        if os.path.exists(f'{directory}/{name}.json'): #if the file exists,\n",
    "            self.logger.debug(f'Reading JSON from {directory}/{filename}') \n",
    "            with open(f'{directory}/{name}.json') as file: #then open it\n",
    "                data=json.load(file) #save it\n",
    "                return data #return it\n",
    "        else:\n",
    "            self.logger.error(f'Trying to open {filename} as JSON')\n",
    "    def Read_List(self,directory,filename): \n",
    "        name, ext=os.path.splitext(filename)\n",
    "        if os.path.exists(f'{directory}/{name}.pkl'): #if the file exists,\n",
    "            self.logger.debug(f'Reading PKL from {directory}/{filename}')\n",
    "            with open(f'{directory}/{name}.pkl','rb') as file: #then open it\n",
    "                data=pickle.load(file) #save it\n",
    "                return data #return it\n",
    "        else:\n",
    "            self.logger.error(f'Trying to open {filename} as PKL')\n",
    "    def Write_JSON(self,dictionary,directory,filename): #deprecated\n",
    "        name, ext=os.path.splitext(filename)\n",
    "        if ext!='.json': #make sure we are writing a json\n",
    "            if ext!='':\n",
    "                self.logger.error(f'Trying to write {filename} as {ext} is not JSON')\n",
    "                return\n",
    "        os.makedirs(directory,exist_ok=True) #make sure the directory exists, if it does not, create it\n",
    "        self.logger.debug(f'Writing JSON at {directory}/{name}.json')\n",
    "        with open(f'{directory}/{name.replace(':','')}.json','w') as file:  #create a file\n",
    "            json.dump(dictionary,file,indent=4) #dump the dictionary in the file\n",
    "    def Write_List(self,list,directory,filename):\n",
    "        name, ext=os.path.splitext(filename)\n",
    "        if ext!='.pkl': #make sure we are writing a pkl\n",
    "            if ext!='':\n",
    "                self.logger.error(f'Trying to write {filename} as {ext} is not PKL')\n",
    "                return\n",
    "        os.makedirs(directory,exist_ok=True) #make sure the directory exists, if it does not, create it\n",
    "        self.logger.debug(f'Writing PKL at {directory}/{name}.pkl')\n",
    "        with open(f'{directory}/{name.replace(':','')}.pkl','wb') as file:  #create a file\n",
    "            pickle.dump(list,file) #dump the list in the file\n",
    "\n",
    "    def Remove_Reserved_IPs(self,ips):\n",
    "        self.logger.debug(f'Removing reserved IP ranges from list')\n",
    "        dot=re.escape('.')\n",
    "        patterns=[f'0{dot}0{dot}0{dot}0', f'10{dot}', f'127{dot}', f'169{dot}254{dot}', f'192{dot}0{dot}0{dot}', f'192{dot}0{dot}2{dot}', f'192{dot}18{dot}', f'192{dot}19{dot}',f'192{dot}51{dot}100{dot}', f'192{dot}88{dot}99{dot}', f'203{dot}0{dot}113{dot}', f'255{dot}', f'256{dot}']+[f'100{dot}{x}{dot}' for x in range(64,128)]+[f'172{dot}{x}{dot}' for x in range(16,32)]+[f'{x}{dot}' for x in range(224,240)]\n",
    "        #self.logger.debug(patterns)\n",
    "        temp=[]\n",
    "        for ip in ips:\n",
    "            for pattern in patterns:\n",
    "                if re.match(pattern,ip):\n",
    "                    self.logger.debug(f'Pruning {ip} with {pattern}')\n",
    "                    temp.append(ip)\n",
    "        for ip in ips:\n",
    "            if re.search('::',ip):\n",
    "                self.logger.debug(f'Pruning IPv6 IP {ip}')\n",
    "                temp.append(ip)\n",
    "        for temp_ip in temp:\n",
    "            ips.remove(temp_ip)\n",
    "        self.logger.debug(f'Removed {temp}')\n",
    "        return ips        \n",
    "    def Calc_Trans_Time(self,pcap_data=stored_pcap_data,forced=False,verbose=True):\n",
    "        time=[]\n",
    "        directory=self.stored_directory[0]\n",
    "        filename='time.pkl'\n",
    "        data_not_found=False\n",
    "        if not forced: #load existing data if found\n",
    "            if os.path.exists(f'{directory}/{filename}'): #if existing data is found\n",
    "                self.logger.info(f'Found existing time data at {directory}/{filename}')\n",
    "                read_list=self.Read_List(directory,filename)\n",
    "                self.List_Initializer(self.stored_unique_ips,read_list)\n",
    "                if verbose:\n",
    "                    print(f'Found time data. [{read_list}]')\n",
    "                return read_list\n",
    "            else:\n",
    "                data_not_found=True\n",
    "        if (forced or data_not_found): #generate data\n",
    "            self.logger.debug(f'Calculating transmission time.')\n",
    "            time=math.floor(pcap_data[-1].time-pcap_data[0].time)\n",
    "            self.Write_List(time,directory,filename)\n",
    "            self.List_Initializer(self.stored_transmission_time,time)\n",
    "            return time\n",
    "\n",
    "    def PCAP_Input(self,path):\n",
    "\n",
    "        file_size=math.ceil(os.path.getsize(path)/math.pow(1024,2))\n",
    "        print(f'Reading PCAP Data. [{file_size} MB]')\n",
    "        self.logger.info(f'Reading PCAP data [{file_size} MB] from path, \"{path}\"')\n",
    "        pcap_data=rdpcap(path)\n",
    "        self.List_Initializer(self.stored_pcap_data,pcap_data)\n",
    "        self.logger.debug(f'Stored {path} data in stored_pcap_data.')\n",
    "        return pcap_data\n",
    "\n",
    "    def Generate_Unique_IPs(self,pcap_data=stored_pcap_data,self_ips=stored_self_ips,remove_reserved=True,verbose=True,forced=False,save=True):\n",
    "        unique_ips=[]\n",
    "        directory=self.stored_directory[0]\n",
    "        filename='unique_ips.pkl'\n",
    "        data_not_found=False\n",
    "        if not forced: #load existing data if found\n",
    "            if os.path.exists(f'{directory}/{filename}'): #if existing data is found\n",
    "                self.logger.info(f'Found existing unique_ips data at {directory}/{filename}')\n",
    "                read_list=self.Read_List(directory,filename)\n",
    "                self.List_Initializer(self.stored_unique_ips,read_list)\n",
    "                if verbose:\n",
    "                    print(f'Found unique_ips data. [{len(read_list)}]')\n",
    "                return read_list\n",
    "            else:\n",
    "                data_not_found=True\n",
    "        if (forced or data_not_found): #generate data\n",
    "            self.logger.info('Extracting Unique IPs from PCAP Data.')\n",
    "            blocked_ips=self_ips.copy()\n",
    "            packets=pcap_data\n",
    "            if verbose:\n",
    "                for packet in tqdm(packets,desc='Analyzing packets for unique IPs'):\n",
    "                    flag=0\n",
    "                    try:\n",
    "                        for ip in blocked_ips:\n",
    "                            if packet.payload.dst==ip:\n",
    "                                flag=1\n",
    "                        if flag==0:\n",
    "                            blocked_ips.append(packet.payload.dst)\n",
    "                            unique_ips.append(packet.payload.dst)\n",
    "                    except AttributeError:\n",
    "                        broadcast_temp='256.256.256.256'\n",
    "                        for ip in blocked_ips:\n",
    "                            if ip==broadcast_temp:\n",
    "                                flag=1\n",
    "                        if flag==0:\n",
    "                            blocked_ips.append(broadcast_temp)\n",
    "                            unique_ips.append(broadcast_temp)\n",
    "            else:\n",
    "                for packet in packets:\n",
    "                    flag=0\n",
    "                    try:\n",
    "                        for ip in blocked_ips:\n",
    "                            if packet.payload.dst==ip:\n",
    "                                flag=1\n",
    "                        if flag==0:\n",
    "                            blocked_ips.append(packet.payload.dst)\n",
    "                            unique_ips.append(packet.payload.dst)\n",
    "                    except AttributeError:\n",
    "                        broadcast_temp='256.256.256.256'\n",
    "                        for ip in blocked_ips:\n",
    "                            if ip==broadcast_temp:\n",
    "                                flag=1\n",
    "                        if flag==0:\n",
    "                            blocked_ips.append(broadcast_temp)\n",
    "                            unique_ips.append(broadcast_temp)\n",
    "            if remove_reserved==True:\n",
    "                self.Remove_Reserved_IPs(unique_ips)\n",
    "            if save==True:\n",
    "                self.Write_List(unique_ips,directory,'unique_ips')\n",
    "            self.List_Initializer(self.stored_unique_ips,unique_ips)\n",
    "        self.logger.debug('Stored unique IPs in stored_unique_ips.')\n",
    "        return unique_ips\n",
    "    def Analyze_Flow(self,pcap_data=stored_pcap_data,forced=False,verbose=True):\n",
    "        directory=self.stored_directory[0]\n",
    "        filename='flow.pkl'\n",
    "        data_not_found=False\n",
    "        \n",
    "        if not forced: #load existing data if found\n",
    "            if os.path.exists(f'{directory}/{filename}'): #if existing data is found\n",
    "                self.logger.info(f'Found existing flow data at {directory}/{filename}')\n",
    "                read_list=self.Read_List(directory,filename)\n",
    "                self.List_Initializer(self.stored_flow,read_list)\n",
    "                if verbose:\n",
    "                    print(f'Found analyzed flow data. [{len(read_list)}]')\n",
    "                return read_list\n",
    "            else:\n",
    "                data_not_found=True\n",
    "        if (forced or data_not_found):\n",
    "            firstmost_pkt_time=pcap_data[0].time\n",
    "            analyzed_flow=[]\n",
    "            trans_time=self.stored_transmission_time[0]\n",
    "            self.logger.info(f'Calculating flow [{len(pcap_data)}]')\n",
    "            if verbose:\n",
    "                print(f'Analyzing packets. [{len(pcap_data)}]')\n",
    "            unique_ips=copy.deepcopy(self.stored_unique_ips)\n",
    "            for ip in self.Remove_Reserved_IPs(unique_ips): #initialize with all the unique ips except self ips\n",
    "                analyzed_flow.append([ip,0,0,0,0,[],[],[],(-1),0,0,0,[0],[0],[0]]) \n",
    "                #IP, Down Bytes, Up Bytes, Down Packets, Up Packets, Used Protocols, S Ports, D Ports, First Packet Time, Last Mixed Packet Time, Last Dl Pkt Time, Last Up Pkt Time, IAT Mixed, IAT Down, IAT Up\n",
    "            \n",
    "            for packet in pcap_data: #for every packet, the analysis of the flow of data happens in this loop\n",
    "                for self_ip in self.stored_self_ips: #for every self ip\n",
    "                    try:\n",
    "                        if packet.payload.dst==self_ip: #check if payload destination is self ip (download)\n",
    "                            for flow_data in analyzed_flow: #for every unique ip tuple\n",
    "                                try:\n",
    "                                    ip=flow_data[0]\n",
    "                                    if ip==packet.payload.src: #if the payload source matches with the unique ip\n",
    "                                        if True: #variable assignment for better clarification and assignment\n",
    "                                            downloaded_bytes=flow_data[1]\n",
    "                                            uploaded_bytes=flow_data[2]\n",
    "                                            downloaded_packets=flow_data[3]\n",
    "                                            uploaded_packets=flow_data[4]\n",
    "                                            protocol_list=flow_data[5]\n",
    "                                            source_ports=flow_data[6]\n",
    "                                            destination_ports=flow_data[7]\n",
    "                                            first_packet_time=flow_data[8]\n",
    "                                            last_mixed_packet_time=flow_data[9]\n",
    "                                            last_downloaded_packet_time=flow_data[10]\n",
    "                                            last_uploaded_packet_time=flow_data[11]\n",
    "                                            iat_mixed=flow_data[12]\n",
    "                                            iat_download=flow_data[13]\n",
    "                                            iat_upload=flow_data[14]\n",
    "                                        downloaded_bytes+=packet.payload.len #add the packet length/size to stored downloaded bytes, payload size is 14 less than the packet size\n",
    "                                        downloaded_packets+=1 #count the number of packets downloaded\n",
    "                                        if packet.payload.proto not in protocol_list: #if the protocol is not already in the list, add it to the list\n",
    "                                            protocol_list.append(packet.payload.proto)\n",
    "                                        if packet.payload.sport not in source_ports: #if the source port is not already in the list, add it to the list\n",
    "                                            source_ports.append(packet.payload.sport)\n",
    "                                        if packet.payload.dport not in destination_ports: #if the destination port is not already in the list, add it to the list\n",
    "                                            destination_ports.append(packet.payload.dport)\n",
    "                                        if first_packet_time==(-1): #find the first packet time\n",
    "                                            first_packet_time=packet.time-firstmost_pkt_time\n",
    "                                        if (iat_download==[0] and downloaded_packets<=1): #if IAT Download list is empty and it is the first packet, pass the first time and note the time, so the first entry is zero, then start\n",
    "                                            last_downloaded_packet_time=packet.time-firstmost_pkt_time #note the downloaded packet's relative time\n",
    "                                        else:\n",
    "                                            iat_download.append(float(packet.time-firstmost_pkt_time-last_downloaded_packet_time)) #calculate the IAT of the following mixed packets\n",
    "                                            last_downloaded_packet_time=packet.time-firstmost_pkt_time #keep renoting the mixed packet's relative time\n",
    "                                        if (iat_mixed==[0] and downloaded_packets<=1): #if IAT Mixed list is empty and it is the first packet, pass the first time and note the time, so the first entry is zero, then start\n",
    "                                            last_mixed_packet_time=packet.time-firstmost_pkt_time #note the mixed packet's relative time\n",
    "                                        else:\n",
    "                                            iat_mixed.append(float(packet.time-firstmost_pkt_time-last_mixed_packet_time)) #calculate the IAT of the following mixed packets\n",
    "                                            last_mixed_packet_time=packet.time-firstmost_pkt_time #keep renoting the mixed packet's relative time\n",
    "                                        if True: #the non-list variables must be re-assigned here cause they are non-mutable\n",
    "                                            flow_data[1]=downloaded_bytes\n",
    "                                            flow_data[2]=uploaded_bytes\n",
    "                                            flow_data[3]=downloaded_packets\n",
    "                                            flow_data[4]=uploaded_packets\n",
    "                                            flow_data[8]=first_packet_time\n",
    "                                            flow_data[9]=last_mixed_packet_time\n",
    "                                            flow_data[10]=last_downloaded_packet_time\n",
    "                                            flow_data[11]=last_uploaded_packet_time\n",
    "                                        break\n",
    "                                except AttributeError:\n",
    "                                    pass\n",
    "                            break\n",
    "                        elif packet.payload.src==self_ip: #if the destination isnt a self ip, then it must be upload\n",
    "                            for flow_data in analyzed_flow: #for every unique ip\n",
    "                                try:\n",
    "                                    ip=flow_data[0]\n",
    "                                    if ip==packet.payload.dst: #if the payload destination matches with the unique ip\n",
    "                                        if True: #variable assignment for better clarification and assignment\n",
    "                                            downloaded_bytes=flow_data[1]\n",
    "                                            uploaded_bytes=flow_data[2]\n",
    "                                            downloaded_packets=flow_data[3]\n",
    "                                            uploaded_packets=flow_data[4]\n",
    "                                            protocol_list=flow_data[5]\n",
    "                                            source_ports=flow_data[6]\n",
    "                                            destination_ports=flow_data[7]\n",
    "                                            first_packet_time=flow_data[8]\n",
    "                                            last_mixed_packet_time=flow_data[9]\n",
    "                                            last_downloaded_packet_time=flow_data[10]\n",
    "                                            last_uploaded_packet_time=flow_data[11]\n",
    "                                            iat_mixed=flow_data[12]\n",
    "                                            iat_download=flow_data[13]\n",
    "                                            iat_upload=flow_data[14]\n",
    "                                        uploaded_bytes+=packet.payload.len #add the packet length/size to stored uploaded bytes, payload size is 14 less than the packet size\n",
    "                                        uploaded_packets+=1 #count the number of packets uploaded\n",
    "                                        if packet.payload.proto not in protocol_list: #if the protocol is not already in the list, add it to the list\n",
    "                                            protocol_list.append(packet.payload.proto)\n",
    "                                        if packet.payload.sport not in source_ports: #if the source port is not already in the list, add it to the list\n",
    "                                            source_ports.append(packet.payload.sport)\n",
    "                                        if packet.payload.dport not in destination_ports: #if the destination port is not already in the list, add it to the list\n",
    "                                            destination_ports.append(packet.payload.dport)\n",
    "                                        if first_packet_time==(-1): #find the first packet time\n",
    "                                            first_packet_time=packet.time-firstmost_pkt_time\n",
    "                                        if (packet.time-firstmost_pkt_time) > last_mixed_packet_time: #find the last mixed packet time\n",
    "                                            last_mixed_packet_time=packet.time-firstmost_pkt_time\n",
    "                                        if (iat_upload==[0] and uploaded_packets<=1): #if IAT Upload list is empty and it is the first packet, pass the first time and note the time, so the first entry is zero, then start\n",
    "                                            last_uploaded_packet_time=packet.time-firstmost_pkt_time #note the uploaded packet's relative time\n",
    "                                        else:\n",
    "                                            iat_upload.append(float(packet.time-firstmost_pkt_time-last_uploaded_packet_time)) #calculate the IAT of the following uploaded packets\n",
    "                                            last_uploaded_packet_time=packet.time-firstmost_pkt_time #keep renoting the uploaded packet's relative time\n",
    "                                        if (iat_mixed==[0] and uploaded_packets<=1): #if IAT Mixed list is empty and it is the first packet, pass the first time and note the time, so the first entry is zero, then start\n",
    "                                            last_mixed_packet_time=packet.time-firstmost_pkt_time #note the mixed packet's relative time\n",
    "                                        else:\n",
    "                                            iat_mixed.append(float(packet.time-firstmost_pkt_time-last_mixed_packet_time)) #calculate the IAT of the following mixed packets\n",
    "                                            last_mixed_packet_time=packet.time-firstmost_pkt_time #keep renoting the mixed packet's relative time\n",
    "                                        if True: #the non-list variables must be re-assigned here cause they are non-mutable\n",
    "                                            flow_data[1]=downloaded_bytes\n",
    "                                            flow_data[2]=uploaded_bytes\n",
    "                                            flow_data[3]=downloaded_packets\n",
    "                                            flow_data[4]=uploaded_packets\n",
    "                                            flow_data[8]=first_packet_time\n",
    "                                            flow_data[9]=last_mixed_packet_time\n",
    "                                            flow_data[10]=last_downloaded_packet_time\n",
    "                                            flow_data[11]=last_uploaded_packet_time\n",
    "                                        break\n",
    "                                except AttributeError: #if removed, broadcast packets will throw errors\n",
    "                                    pass\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "            for flow_data in analyzed_flow: #2nd iteration of analysis happens here\n",
    "                dl_bytes=flow_data[1]\n",
    "                up_bytes=flow_data[2]\n",
    "                dl_pkts=flow_data[3]\n",
    "                up_pkts=flow_data[4]\n",
    "                first_pkt_time=flow_data[8]\n",
    "                last_pkt_time=flow_data[9]\n",
    "                iats_mixed=flow_data[12]\n",
    "                iats_down=flow_data[13]\n",
    "                iats_up=flow_data[14]\n",
    "                flow_data.append(self.Dec_Trunc(dl_bytes/trans_time,1000)) #avg download speed in bytes/s\n",
    "                flow_data.append(self.Dec_Trunc(up_bytes/trans_time,1000)) #avg upload speed in bytes/s\n",
    "                try:\n",
    "                    down_psize=self.Dec_Trunc(dl_bytes/dl_pkts,1000)\n",
    "                except ZeroDivisionError:\n",
    "                    down_psize=0\n",
    "                try:\n",
    "                    up_psize=self.Dec_Trunc(up_bytes/up_pkts,1000)\n",
    "                except ZeroDivisionError:\n",
    "                    up_psize=0\n",
    "                flow_duration=self.Dec_Trunc(last_pkt_time-first_pkt_time,1000)\n",
    "                try:\n",
    "                    flow_rate=self.Dec_Trunc((dl_bytes+up_bytes)/flow_duration,1000)\n",
    "                except ZeroDivisionError:\n",
    "                    flow_rate=0\n",
    "                burstiness_mixed=np.std(iats_mixed)/np.mean(iats_mixed)\n",
    "                burstiness_down=np.std(iats_down)/np.mean(iats_down)\n",
    "                burstiness_up=np.std(iats_up)/np.mean(iats_up)\n",
    "                if np.isnan(burstiness_mixed):\n",
    "                    burstiness_mixed=np.float64(0)\n",
    "                if np.isnan(burstiness_down):\n",
    "                    burstiness_down=np.float64(0)\n",
    "                if np.isnan(burstiness_up):\n",
    "                    burstiness_up=np.float64(0)\n",
    "                flow_data.append(burstiness_mixed)\n",
    "                flow_data.append(burstiness_down)\n",
    "                flow_data.append(burstiness_up)\n",
    "                flow_data.append(flow_duration)\n",
    "                flow_data.append(flow_rate)\n",
    "                flow_data.append(down_psize)\n",
    "                flow_data.append(up_psize)\n",
    "            self.List_Initializer(self.stored_flow,analyzed_flow)\n",
    "            directory=self.stored_directory[0]\n",
    "            self.Write_List(analyzed_flow,directory,filename)\n",
    "        self.logger.debug('Stored total flow in stored_flow')\n",
    "        return analyzed_flow\n",
    "    \n",
    "    def Query_IP(self,ip):\n",
    "        self.logger.debug(f'Querying for IP {ip}')\n",
    "        response=requests.get(f\"http://ip-api.com/json/{ip}\")\n",
    "        data=response.json()\n",
    "        return data    \n",
    "    def Query_IPs(self,ips=stored_unique_ips,amount=45,forced=False,verbose=True):\n",
    "        queried_data=[]\n",
    "        to_be_queried=ips.copy()\n",
    "        directory=self.stored_directory[0]\n",
    "        filename='queried_data.pkl'\n",
    "        incomplete=False\n",
    "        data_not_found=False\n",
    "        if not forced: #load existing data if found\n",
    "            if os.path.exists(f'{directory}/{filename}'): #if existing data is found\n",
    "                self.logger.info(f'Found existing query data at {directory}/{filename}')\n",
    "                read_list=self.Read_List(directory,filename)\n",
    "                self.List_Initializer(queried_data,read_list)\n",
    "                if len(read_list)<len(to_be_queried):\n",
    "                    incomplete=True\n",
    "                if verbose:\n",
    "                    print(f'Found query data. [{len(queried_data)}]')\n",
    "                    if incomplete:\n",
    "                        print(f'Query data is incomplete. [{min(len(to_be_queried)-len(read_list),amount)}+{max(0,len(to_be_queried)-len(read_list)-amount)}]')\n",
    "            else:\n",
    "                data_not_found=True\n",
    "        if (forced or incomplete or data_not_found):\n",
    "            if incomplete: #if incomplete, prune\n",
    "                self.logger.debug(f'Pruning IPs')\n",
    "                prune=[]\n",
    "                for query in queried_data: #for every queried IP\n",
    "                    for ip in to_be_queried: #for every IP to be queried\n",
    "                        if query[0]==ip: #check if the IP has already been queried\n",
    "                            prune.append(ip) #add the IP to a pruning list\n",
    "                            break\n",
    "                for pruning_ip in prune: #prune the IPs\n",
    "                    to_be_queried.remove(pruning_ip)\n",
    "            self.logger.info(f'Querying IPs [{len(to_be_queried)}]')\n",
    "            if verbose:\n",
    "                for ip in tqdm(to_be_queried[:min(amount,45)],desc='Querying IPs'):\n",
    "                    response=self.Query_IP(ip)\n",
    "                    temp=[ip]\n",
    "                    temp.append(response)\n",
    "                    queried_data.append(temp)\n",
    "            else:\n",
    "                print(f'Query_IPs verbose=false incomplete')\n",
    "            self.Write_List(queried_data,directory,filename)\n",
    "        self.List_Initializer(self.stored_queried_data,queried_data)\n",
    "        self.logger.debug('Stored queried data in stored_queried_data')\n",
    "        \n",
    "        return queried_data\n",
    "    \n",
    "\n",
    "        if self.stored_flow!=[]: #if there is flow data, add it to summary\n",
    "            for summary in summarized:\n",
    "                for flow_data in self.stored_flow:\n",
    "                    if flow_data[0]==summary['IP']:\n",
    "                        try:\n",
    "                            down_psize=math.trunc(flow_data[1]/flow_data[3]*100)/100\n",
    "                        except ZeroDivisionError:\n",
    "                            down_psize=0\n",
    "                        try:\n",
    "                            up_psize=math.trunc(flow_data[2]/flow_data[4]*100)/100\n",
    "                        except ZeroDivisionError:\n",
    "                            up_psize=0\n",
    "                        summary['Flow']=f'{flow_data[5]} byte/s Down, {flow_data[6]} byte/s Up. Packets, {flow_data[3]} Received, {flow_data[4]} Sent'\n",
    "                        summary['Misc']=f'Average Packet Size {down_psize} Down, {up_psize} Up'\n",
    "                            \n",
    "        return summarized\n",
    "\n",
    "    def Generate_Dataset(self,data_in=stored_flow,replace=True,ping=True,forced=False,verbose=True):\n",
    "        directory=self.stored_directory[0]\n",
    "        filename='dataframe.csv'\n",
    "        data_not_found=False\n",
    "        data=copy.deepcopy(data_in)\n",
    "        if not forced: #load existing data if found\n",
    "            if os.path.exists(f'{directory}/{filename}'): #if existing data is found\n",
    "                self.logger.info(f'Found existing dataframe at {directory}/{filename}')\n",
    "                df=pd.read_csv(f'{directory}/{filename}')\n",
    "                if verbose:\n",
    "                    print(f'Found dataframe. [{len(df)}]')\n",
    "            else:\n",
    "                data_not_found=True\n",
    "        if (forced or data_not_found):\n",
    "            protocols=self.stored_protocol_dict\n",
    "            ports=self.stored_port_dict\n",
    "            self.logger.info(f'Generating dataset')\n",
    "            if verbose:\n",
    "                print(f'Generating Dataset [{len(data)}]')\n",
    "            if replace:\n",
    "                for flow_data in data:\n",
    "                    protocol_data=flow_data[5]\n",
    "                    sport_data=flow_data[6]\n",
    "                    sport_prune=[]\n",
    "                    dport_data=flow_data[7]\n",
    "                    dport_prune=[]\n",
    "                    for protocol in protocol_data:\n",
    "                        if str(protocol) in protocols:\n",
    "                            protocol_data[protocol_data.index(protocol)]=protocols.get(str(protocol))\n",
    "                        else:\n",
    "                            self.logger.error(f'Protocol {protocol} not found in dictionary')\n",
    "                    for sport in sport_data:\n",
    "                        if str(sport) in ports:\n",
    "                            sport_data[sport_data.index(sport)]=ports.get(str(sport))\n",
    "                        elif sport>=49152 and sport<=65535: #all the dynamic ports\n",
    "                            if 'Dynamic' not in sport_data: #if a dynamic port doesnt already exist, make one, rest go in garbage\n",
    "                                sport_data[sport_data.index(sport)]='Dynamic'\n",
    "                            else:\n",
    "                                sport_prune.append(sport) #add all the dynamic ports to a pruning list\n",
    "                        elif sport>=27000 and sport<=27038: #steam ports\n",
    "                            if 'Steam' not in sport_data:\n",
    "                                sport_data[sport_data.index(sport)]='Steam'\n",
    "                            else:\n",
    "                                sport_prune.append(sport)\n",
    "                        else:\n",
    "                            self.logger.debug(f'Port {sport} not found in dictionary')\n",
    "                    for dport in dport_data:\n",
    "                        if str(dport) in ports:\n",
    "                            dport_data[dport_data.index(dport)]=ports.get(str(dport))\n",
    "                        elif dport>=49152 and dport<=65535: #all the dynamic ports\n",
    "                            if 'Dynamic' not in dport_data: #if a dynamic port doesnt already exist, make one, rest go in garbage\n",
    "                                dport_data[dport_data.index(dport)]='Dynamic'\n",
    "                            else:\n",
    "                                dport_prune.append(dport) #add all the dynamic ports to a pruning list\n",
    "                        elif dport>=27000 and dport<=27038: #steam ports\n",
    "                            if 'Steam' not in dport_data:\n",
    "                                dport_data[dport_data.index(dport)]='Steam'\n",
    "                            else:\n",
    "                                sport_prune.append(dport)\n",
    "                        else:\n",
    "                            self.logger.debug(f'Port {dport} not found in dictionary')\n",
    "                    for sport in sport_prune: #prune the ports\n",
    "                        sport_data.remove(sport)\n",
    "                    for dport in dport_prune: #prune the ports\n",
    "                        dport_data.remove(dport)\n",
    "            df=pd.DataFrame(data,columns=['IP Address','Bytes Down','Bytes Up','Pkts Down','Pkts Up','Protocol(s)','Source Ports','Destination Ports',\n",
    "                                          'First Pkt Time','Last Down Pkt Time','Last Up Pkt Time','Last Mixed Pkt Time',\n",
    "                                          'IAT Mixed','IAT Down','IAT Up',\n",
    "                                          'Avg Down B/s','Avg Up B/s',\n",
    "                                          'Burst Mixed','Burst Down','Burst Up',\n",
    "                                          'Flow Dura','Flow Rate',\n",
    "                                          'Avg Down PSize','Avg Up PSize'])\n",
    "            if self.stored_queried_data!=[]:\n",
    "                to_be_appended=[]\n",
    "                for query_data in self.stored_queried_data:\n",
    "                    temp={}\n",
    "                    query=query_data[1]\n",
    "                    if query['status']=='success':\n",
    "                        temp['Organization']=f\"{query['as']}, {query['org']}, {query['isp']}\"\n",
    "                        temp['Location']=f\"{query['city']}, {query['regionName']}, {query['country']} | {query['lat']}, {query['lon']}\"\n",
    "                    elif query['status']=='fail':\n",
    "                        temp['Location']=f'Unknown'\n",
    "                        temp['Organization']=f'Unknown'\n",
    "                    to_be_appended.append(temp)\n",
    "                q_df=pd.DataFrame(to_be_appended)\n",
    "                df=pd.concat([df,q_df],axis=1)\n",
    "            if ping:\n",
    "                count=10\n",
    "                timeout=1000\n",
    "                packet_size=32\n",
    "                processes=[]\n",
    "                outputs=[]\n",
    "                if verbose:\n",
    "                    print(f'Pinging IPs [{len(data)}]')\n",
    "                for flow_data in data:\n",
    "                    ip=flow_data[0]\n",
    "                    processes.append(subprocess.Popen([\"ping\",ip,\"-n\",str(count),\"-w\",str(timeout),\"-l\",str(packet_size)],\n",
    "                                                      stdout=subprocess.PIPE,text=True,shell=True))\n",
    "                time.sleep(min(int(count*(timeout/1000)*(1+len(data)*0.001)),int(count*5)))\n",
    "                for process in processes:\n",
    "                    outputs.append(process.stdout)\n",
    "                ping_data=[]\n",
    "                for output in outputs:\n",
    "                    loss=\"0%\"\n",
    "                    avg=\"N/A\"\n",
    "                    for line in output:\n",
    "                        string=line.strip()\n",
    "                        if re.search('loss',string):\n",
    "                            loss=string[string.find('(')+1:string.find(')')-5]\n",
    "                        if re.search('Destination host unreachable',string):\n",
    "                            avg=\"DHU\"\n",
    "                        elif re.search('Request timed out',string):\n",
    "                            avg=\"RTO\"\n",
    "                        elif re.search('General failure',string):\n",
    "                            avg=\"GF\"\n",
    "                        elif re.search('Average',string):\n",
    "                            avg=string[string.find('Average = ')+10:]\n",
    "                    temp=[avg,loss]\n",
    "                    ping_data.append(temp)\n",
    "                ping_df=pd.DataFrame(ping_data,columns=['Ping','Loss'])\n",
    "                df=pd.concat([df,ping_df],axis=1)\n",
    "            \n",
    "            df.to_csv(f'{directory}/{filename}',index=False)\n",
    "        self.List_Initializer(self.stored_df,df)\n",
    "        self.logger.debug(f'Stored dataframe in stored_dataframe')\n",
    "        return df\n",
    "        \n",
    "    def Generate_Cluster(self,df_input=stored_df,features=['Burst Down','Burst Up'],clustering_algorithm='kmeans',clusters=6,samples=2,show=True):\n",
    "        df=df_input[0]\n",
    "        scaler=StandardScaler()\n",
    "        features_scaled=scaler.fit_transform(df[features])\n",
    "        if show:\n",
    "            self.logger.info(f'Generating cluster using {clustering_algorithm}')\n",
    "        match clustering_algorithm:\n",
    "            case 'kmeans':\n",
    "                kmeans=KMeans(n_clusters=clusters)\n",
    "                prediction=kmeans.fit_predict(features_scaled)\n",
    "                df[f'Cluster [{clustering_algorithm}]']=prediction\n",
    "            case 'dbscan':\n",
    "                dbscan=DBSCAN(eps=0.5,min_samples=samples)\n",
    "                prediction=dbscan.fit_predict(features_scaled)\n",
    "                df[f'Cluster [{clustering_algorithm}]']=prediction\n",
    "            case 'agglomerative':\n",
    "                agglo=AgglomerativeClustering(n_clusters=clusters)\n",
    "                prediction=agglo.fit_predict(features_scaled)\n",
    "                df[f'Cluster [{clustering_algorithm}]']=prediction\n",
    "            case 'meanshift':\n",
    "                meanshift=MeanShift()\n",
    "                prediction=meanshift.fit_predict(features_scaled)\n",
    "                df[f'Cluster [{clustering_algorithm}]']=prediction\n",
    "            case 'spectral':\n",
    "                spectral=SpectralClustering(n_clusters=clusters, affinity='nearest_neighbors')\n",
    "                prediction=spectral.fit_predict(features_scaled)\n",
    "                df[f'Cluster [{clustering_algorithm}]']=prediction\n",
    "            case 'affinity':\n",
    "                affinity=AffinityPropagation()\n",
    "                prediction=affinity.fit_predict(features_scaled)\n",
    "                df[f'Cluster [{clustering_algorithm}]']=prediction\n",
    "            case 'birch':\n",
    "                birch=Birch(n_clusters=clusters)\n",
    "                prediction=birch.fit_predict(features_scaled)\n",
    "                df[f'Cluster [{clustering_algorithm}]']=prediction\n",
    "            case 'optics':\n",
    "                optics=OPTICS(min_samples=samples, xi=0.05, min_cluster_size=0.1)\n",
    "                prediction=optics.fit_predict(features_scaled)\n",
    "                df[f'Cluster [{clustering_algorithm}]']=prediction\n",
    "                \n",
    "        if show:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            scatter=plt.scatter(df[features[0]],df[features[1]],\n",
    "                                c=df[f'Cluster [{clustering_algorithm}]'],cmap='viridis',s=100,edgecolor='k')\n",
    "            plt.xlabel(f'{features[0]}')\n",
    "            plt.ylabel(f'{features[1]}')\n",
    "            plt.title(f'Network Flow Clustering by {features[0]} and {features[1]}')\n",
    "            plt.colorbar(scatter, label=f'Cluster [{clustering_algorithm}]')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            return\n",
    "        else:\n",
    "            return prediction\n",
    "    def Generate_Ensembling(self,df_input=stored_df,features=['Burst Down','Burst Up'],ensembling_algorithms=['kmeans','dbscan','agglomerative'],clusters=6,samples=2,show=True):\n",
    "        df=df_input[0]\n",
    "        clusterings=[]\n",
    "        self.logger.info(f'Generating cluster ensemble using {ensembling_algorithms}')\n",
    "        for algo in ensembling_algorithms:\n",
    "            clusterings.append(self.Generate_Cluster(features=features,clustering_algorithm=algo,clusters=clusters,samples=samples,show=False))\n",
    "        \n",
    "        n_samples=len(df)\n",
    "        n_clusterings=len(clusterings)\n",
    "        coassoc_matrix=np.zeros((n_samples,n_samples))\n",
    "        \n",
    "        for labels in clusterings:\n",
    "            for i in range(n_samples):\n",
    "                for j in range(n_samples):\n",
    "                    if labels[i] == labels[j]:\n",
    "                        coassoc_matrix[i][j] += 1\n",
    "        \n",
    "        coassoc_matrix/=n_clusterings\n",
    "        dissimilarity_matrix=1-coassoc_matrix\n",
    "        consensus_labels=AgglomerativeClustering(n_clusters=clusters,metric='precomputed',linkage='average').fit_predict(dissimilarity_matrix)  #Dissimilarity= 1-Similarity\n",
    "        df[f'Cluster [CSPA]']=consensus_labels\n",
    "        result=[]\n",
    "        temp=[]\n",
    "        count=0\n",
    "        for algo in clusterings:\n",
    "            ari=adjusted_rand_score(clusterings[count],consensus_labels)\n",
    "            nmi=normalized_mutual_info_score(clusterings[count],consensus_labels)\n",
    "            temp=[]\n",
    "            temp.append(ensembling_algorithms[count])\n",
    "            temp.append(ari)\n",
    "            temp.append(nmi)\n",
    "            result.append(temp)\n",
    "            count+=1\n",
    "        if show:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            scatter=plt.scatter(df[features[0]],df[features[1]],\n",
    "                                c=consensus_labels,cmap='viridis',s=100,edgecolor='k')\n",
    "            plt.xlabel(f'{features[0]}')\n",
    "            plt.ylabel(f'{features[1]}')\n",
    "            plt.title(f'Network Flow Clustering by {features[0]} and {features[1]}')\n",
    "            plt.colorbar(scatter, label=f'Cluster [CSPA]')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            return\n",
    "        return result\n",
    "    def Trace_IPs(self,ips=stored_unique_ips,amount=99,hops=20,timeout=1000):\n",
    "        self.logger.info(f'Tracing IPs {ips[:amount]}')\n",
    "        processes=[]\n",
    "        for ip in ips[:amount]:\n",
    "            processes.append(subprocess.Popen([\"tracert\",\"-h\",str(hops),\"-w\",str(timeout),ip],\n",
    "                                              stdout=subprocess.PIPE,text=True,shell=True))\n",
    "        time.sleep(math.ceil(hops*(timeout/1000)*3.75*(1+amount*0.001)))\n",
    "        outputs_temp=[]\n",
    "        for process in processes:\n",
    "            outputs_temp.append(process.stdout)\n",
    "        output_cleaned=[]\n",
    "        for output in outputs_temp:\n",
    "            temp=[]\n",
    "            for line in output:\n",
    "                if line.strip()!='':\n",
    "                    temp.append(line.strip())\n",
    "            output_cleaned.append(temp)\n",
    "        return output_cleaned\n",
    "    \n",
    "    def Analyze_Flow_Rate(self,pcap_data=stored_pcap_data,forced=False,verbose=True): #deprecated\n",
    "        directory=self.stored_directory[0]\n",
    "        filename='flow_rate.pkl'\n",
    "        data_not_found=False\n",
    "        if not forced: #load existing data if found\n",
    "            if os.path.exists(f'{directory}/{filename}'): #if existing data is found\n",
    "                self.logger.info(f'Found existing flow_rate data at {directory}/{filename}')\n",
    "                read_list=self.Read_List(directory,filename)\n",
    "                self.List_Initializer(self.stored_flow_rate,read_list)\n",
    "                if verbose:\n",
    "                    print(f'Found flow_rate data. [{len(read_list)}]')\n",
    "                return read_list\n",
    "            else:\n",
    "                data_not_found=True\n",
    "        if (forced or data_not_found):\n",
    "            self.logger.info('Calculating flow rates')\n",
    "            trans_time=self.stored_transmission_time[0]\n",
    "            flow=self.Analyze_Flow(pcap_data)\n",
    "            for ip_flow in flow:\n",
    "                ip_flow[1]=math.trunc(float(ip_flow[1])/trans_time*100)/100\n",
    "                ip_flow[2]=math.trunc(float(ip_flow[2])/trans_time*100)/100\n",
    "            flow_rate=flow\n",
    "            self.List_Initializer(self.stored_flow_rate,flow_rate)\n",
    "            self.Write_List(flow_rate,self.stored_directory[0],'flow_rate')\n",
    "        self.logger.debug('Stored flow rate in stored_flow_rate')\n",
    "        return flow_rate\n",
    "    def Query_Unique_IPs(self,unique_ips=stored_unique_ips,amount=20): #deprecated\n",
    "        directory=self.stored_directory[0]\n",
    "        if os.path.exists(f'{directory}/'):\n",
    "            pass\n",
    "        queried_unique_ips=[]\n",
    "        to_be_queried=unique_ips.copy()\n",
    "        \n",
    "        self.logger.info(f'Querying unique IPs.')\n",
    "        \n",
    "        \n",
    "        if len(self.stored_queried_ips)!=0: #if data already exists, then read it\n",
    "            self.logger.debug(f'Found queried data. Pruning query list')\n",
    "            for query in self.stored_queried_ips:   #prune the IPs to be queried\n",
    "                for ip in to_be_queried:\n",
    "                    try:\n",
    "                        if query['query']==ip:\n",
    "                            to_be_queried.remove(ip)\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "        if len(to_be_queried)!=0: #only if there are IPs to be queried, then query\n",
    "            for ip in tqdm(to_be_queried[:min(amount,45)],desc='Querying IPs'):\n",
    "                response=self.Query_IP(ip)\n",
    "                queried_unique_ips.append(response)\n",
    "                self.Write_JSON(response,self.stored_path[0][:self.stored_path[0].find('.')],response['query'])\n",
    "        else:\n",
    "            self.logger.info(f'All unique IPs are queried')\n",
    "            print(f'All unique IPs are already queried')\n",
    "            temp={'status':'Completed',\n",
    "                  'amount':f'{len(self.stored_queried_ips)} IPs',\n",
    "                  'time':f'{self.stored_transmission_time[0]} seconds'}\n",
    "            self.Write_JSON(temp,self.stored_path[0][:self.stored_path[0].find('.')],'status')\n",
    "        if len(self.stored_queried_ips)!=0: #if theres already data then append to it, else initialize it, if removed it will always overwrite data\n",
    "            for query in queried_unique_ips:\n",
    "                self.stored_queried_ips.append(query)\n",
    "        else:\n",
    "            self.List_Initializer(self.stored_queried_ips,queried_unique_ips)\n",
    "        return queried_unique_ips\n",
    "    def Generate_Summary(self,data=stored_queried_data): #deprecated\n",
    "        self.logger.info(f'Summarizing data.')\n",
    "        summarized=[]\n",
    "        for query_data in data:\n",
    "            temp={}\n",
    "            query=query_data[1]\n",
    "            if query['status']=='success':\n",
    "                temp['Organization']=f\"{query['as']}, {query['org']}, {query['isp']}\"\n",
    "                temp['Location']=f\"{query['city']}, {query['regionName']}, {query['country']} | {query['lat']}, {query['lon']}\"\n",
    "                temp['IP']=f\"{query['query']}\"\n",
    "            elif query['status']=='fail':\n",
    "                temp['Location']=f'Unknown [Failed]'\n",
    "                temp['Organization']=f'Unknown [Failed]'\n",
    "                temp['IP']=f\"{query['query']}\"\n",
    "            summarized.append(temp)\n",
    "        self.logger.debug('Summarizing completed.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_features=['Burst Mixed', 'Avg Down PSize']\n",
    "algos=['kmeans','agglomerative']\n",
    "clusters=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=Analyzer('PCAP Data/6400S-Dailies-Discord.pcapng',\n",
    "           input=False,features=temp_features,\n",
    "           query=True,forced_query=False,infinite_query=True,\n",
    "           ping=False,cluster=False,clustering_algorithm='birch',clusters=clusters,samples=4,\n",
    "           forced=False,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.Generate_Ensembling(features=temp_features,ensembling_algorithms=algos,show=False,clusters=clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.Generate_Cluster(features=temp_features,clustering_algorithm=algos[0],clusters=clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.Generate_Cluster(features=temp_features,clustering_algorithm=algos[1],clusters=clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.Generate_Ensembling(features=temp_features,ensembling_algorithms=algos,clusters=clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.stored_df[0].drop(['Destination Ports','Protocol(s)','First Pkt Time','Last Down Pkt Time','Last Up Pkt Time','Last Mixed Pkt Time','IAT Mixed','IAT Down','IAT Up'],axis=1).sort_values('Avg Down PSize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data\n",
    "X, y_true = make_blobs(n_samples=100, centers=4, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            s=200, c='red', marker='X')  # cluster centers\n",
    "plt.title('K-Means Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Sample 3D data\n",
    "X, _ = make_blobs(n_samples=100, centers=4, n_features=3, random_state=42)\n",
    "\n",
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Plotting in 3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels, cmap='viridis', s=50)\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Feature 3')\n",
    "plt.title('3D Clustering')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
